---
title: "uber"
author: "Valeriy Kondruk"
date: "11 07 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Tip Frequency problem

**On which day of the week riders tip most frequently?**
  
That would be great to know if there's any correlation between the day of the week and tip amount and frequency a driver gets.  

We have a data set covering 55 consequtive weeks of driving history. Using this sample, we want to make an inference about the population (all possible weeks for this particular driver). 

### Import and clean data

```{r echo=FALSE, warning=FALSE, results='hide', message=FALSE, error=FALSE}
# plyr is handy for multiple csv import 
library(plyr)

library(dplyr)
library(ggplot2)
library(readxl)
library(readr)

# package lubridate for easy date/time handling. check if the package is installed already. install it if it's not.
if (!require(lubridate)) install.packages("lubridate", repos="http://cran.us.r-project.org")

library(lubridate)
```

*the following code chunks won't run as we already have a data frame ready*  

```{r eval=FALSE, error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
### Getting data from multiple csvs
  
# We have a raw data in Uber weekly statements that we want to put into one data set.

# We first put all csvs into dedicated folder. Then, we create a list of all files in csv folder.

mydir = "csv"
myfiles = list.files(path=mydir, pattern="*.csv", full.names=TRUE)

# We use ldply function from plyr library to build a data frame from multiple csv files using read_csv function from readr library.

uber_trips = ldply(myfiles, read_csv)
```

```{r eval=FALSE, include=FALSE}
# First, we would need to clean the name and an email of the driver for ethical reasons.

# Clean name and email
uber_trips <- uber_trips %>%
  mutate(`Driver Name` = "John Doe", `Email` = "jd@jd.com")

# Save data frame in rds file
saveRDS(uber_trips, file = "uber_trips.rds")
```

--------------------------------------------

*Load data frame*

```{r}
uber_trips <- readRDS(file = "uber_trips.rds")
names(uber_trips)
```

Fortunately, raw data already show a day of the week for each trip, so we don't need to take extra steps to transform date into days of the week. However, we would need to split the 'Date/Time' parameter into 'Date/Time' and 'Weekday'.
  
```{r date_time_separation}
# create new column, convert Date/Time data in a proper format then extract the week day

uber_trips <- uber_trips %>%
  mutate(date_time = as.POSIXct(strptime(uber_trips$`Date/Time`, format = "%A, %B %d, %Y %I:%M %p"))) %>%
  # assign week day: time after midnight (till 5 am) considered a previous day since the shift isn't over 
  mutate(week_day = ifelse(hour(date_time) > 5, weekdays(date_time, abbreviate = TRUE), (weekdays(as.Date(date_time)-1, abbreviate = TRUE))))
```

*Please, note that driver's shift often spans from evening to late night. Thus, for this particular analysis we don't switch the day right after midnight and we keep the same day until 5am in the morning. This way a tip recieved on a Sunday's late night ride considered a Sunday's tip, not Monday's which makes sense.*   


```{r}
range(uber_trips$date_time)
```

We have the weekly data spanning between September 22, 2018 and November 8, 2019. Data for some of the weeks is unavailable as driver didn't drive those weeks. 

All currency columns need to be converted into numerical format.

```{r change_formats}
# create a function which converts char variable into currency variable
currency <- function(x, na.rm = TRUE) (as.numeric(sub('$','',as.character(x),fixed=TRUE)))

# create a list of all columns to be converted
char_list <- colnames(select_if(uber_trips, is.character))
                      
# exclude all non-currency columns
char_list = char_list[- c(1, 2, 3, 4, 5, 20)]
char_list[15] = "Promotions"
char_list[16] = "Cleaning Repairs"
  
# change the format for currency columns. We use new data frame to avoid any data loss
uber_trips_clean <- uber_trips %>%
  mutate_at(char_list, currency, na.rm = TRUE)

# order data frame by date_time (ascending order)
uber_trips_clean <- arrange(uber_trips_clean, date_time)
```

  
### Summary statistics

```{r}
summarize(uber_trips_clean, total_trips = n(), total_per_trip = mean(Total), tip_per_trip = sum(Tip, na.rm = TRUE)/n(), tip_frequency = sum(!is.na(Tip))/n())
```
  
There is a problem with the statistics above: the trips that have been cancelled by either driver or rider are counted as regular trips and add up to the total number of rides. However, no tip can be given for a cancelled ride. This means, that the values above are slightly lower than the real values. Let's filter out those cancelled trips to get more accurate statistics.

```{r}
uber_trips_clean %>%
  filter(is.na(Cancellation)) %>%
  summarize(total_trips = n(), total_per_trip = mean(Total), tip_per_trip = sum(Tip, na.rm = TRUE)/n(), tip_frequency = sum(!is.na(Tip))/n())
```

In fact, this driver on average made $7.71 per trip and almost 80 cents per trip in tips for the period reported. Compare this to 60 cents for 5-star drivers reported by Chandar et al. 
  
Check summary statistics and parameter distributions.

```{r summary_statistics}
# Summary stats for Tip variable
uber_trips_clean %>% 
  filter(!is.na(Tip)) %>%
  summarise(trips_w_tips = n(), tip_mean = mean(as.numeric(Tip,  na.rm = TRUE)), tip_min = min(as.numeric(Tip)), tip_max = max(as.numeric(Tip,  na.rm = TRUE)), tip_sd = sd(as.numeric(Tip))) %>%
  show()

uber_trips_clean %>%
  filter(!is.na(Tip)) %>%
  ggplot(aes(x = Tip)) +
  geom_histogram(binwidth = 3, fill="orange", colour="black") 
```

'Tip' variable represent a right-skewed unimodal distribution with several extreme outliers (more than 3 standard deviations from the mean).

  
### Find dyas when tips received most frequently

```{r}
# Build a frequency plot
uber_trips_clean %>%
  filter(!is.na(Tip)) %>%
  ggplot(aes(y = Tip, x = week_day)) +
  geom_bin2d() +
  scale_fill_gradient(low = "yellow", high="red")

# Number of trips with tip and trips per week day
days_stat <- uber_trips_clean %>%
  filter(is.na(Cancellation)) %>%
  group_by(week_day) %>%
  summarise(trips_w_tip = sum(!is.na(Tip)), trips_total = n(), tip_frequency = trips_w_tip/trips_total)

days_stat

# Tip frequency histogram
days_stat %>%
  ggplot(aes(x = tip_frequency)) +
  geom_histogram(binwidth = 0.015, fill="orange", colour="black")

```
  
## ANOVA hypothesis test
  
From the statistics above, we can see that Tuesdays, Fridays, and Thursdays are the days when riders tip most often (close to 30% of all trips are tipped). However, tip frequency for all week days is quite close. 

It's interesting to check whether the differencies in frequencies are due to chance or not. We'll be using ANOVA test to find out.
  
### Set up hypothesis
  
H0: Average tip frequency is the same around the week  
HA: Average tip frequency differs at least on one day

Significance level (&alpha;) = 0.05
  
### Prepare data
  
All of the observations in our data represent separate rides. To be able to analyze certain days as observations, we need to make some tweaks:

```{r create_shift_day}
# we create a new variable shift_date which differs from the actual date if the trip done after midnight but before 5am (explained earlier)
uber_trips_clean <- uber_trips_clean %>%
  mutate(shift_date = ifelse(hour(date_time) > 5, as.character(date(date_time)), as.character(date(date_time) - 1)))

# combine week day name and shift date to create  
uber_trips_clean$shift <- paste(uber_trips_clean$week_day, uber_trips_clean$shift_date)

# create a separate data frame with shifts observations. Filter for cancelled trips (see above)
uber_shifts <- uber_trips_clean %>%
  filter(is.na(Cancellation)) %>%
  group_by(shift) %>%
  summarise(weekday = first(week_day), trips_w_tip = sum(!is.na(Tip)), trips_total = n(), tip_frequency = trips_w_tip/trips_total)

```

### Check summary statistics

We need to check the form of tip frequency distribution, mean, and variability. 

```{r}
# summary statistics table
uber_shifts %>%
  summarise(days_total = n(), frequency_mean = mean(tip_frequency), frequency_sd = sd(tip_frequency))

# build a histogram for tip frequency
uber_shifts %>%
  ggplot(aes(x = tip_frequency)) +
  geom_histogram(binwidth = 0.075, fill="orange", colour="black")

```

The histogram above shows a slight right skew. There are extreme outliers (over 3 sd's from the mean).

Summary statistics grouped by week days:  

```{r}
# calculate variability within the groups
uber_shifts_summary <- uber_shifts %>%
  group_by(weekday) %>%
  summarise(wday_size = n(), wday_mean = mean(tip_frequency), wday_sd = sd(tip_frequency))

uber_shifts_summary
```

Friday seems to be the day with the highest proportion of trips with tip to the total number of trips (`r round(uber_shifts_summary$wday_mean[5], 3)`). Thursday and Tuesday follow closely (`r round(uber_shifts_summary$wday_mean[1], 3)`). Sunday has the lowest tip to trips ratio of `r round(uber_shifts_summary$wday_mean[4], 3)`.

  
### Check conditions - ANOVA graphical diagnostics
  
#### Independence  
  
Considering the fact that we analyze data for one driver only, it's unclear if daily observations are independent of each other. In case of consequitive days (Thursday, Friday, Saturday, etc.), there's a good chance that they are not independent in terms of driver's behavior, car condition and other factors that influence tipping. However, for example, two Fridays of different weeks (even if consequitive) and months are most likely to be independent. There are not obvious reasons why independence would not hold for most or all observations.

  
#### Normality  
  
The normality assumption is especially important when the sample size is quite small. The normal probability plots for each week day are shown below:

```{r ANOVA_graphical_diagnostics}

# build the normal probability plots for all seven groups
for (i in c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")) {
uber_shifts_days <- uber_shifts %>%
  filter(weekday == i)
qqnorm(uber_shifts_days$tip_frequency, main = i, col="orange")
qqline(uber_shifts_days$tip_frequency)
}

```

* Sun - points follow an S-shaped curve which indicates short tails (narrower than the normal distribution)
* Mon - not clear
* Tue - points start below the line, bend to follow it, and end above it which indicates long tails (wider than the normal distribution)
* Wed - points bend up and to the left of the line - obviously, right skew
* Thu - not clear
* Fri - nearly normal
* Sat - nearly normal  

There are some deviation from normality for most of the week days except Friday and Saturday. The sample sizes are not large. The outliers are not extreme, though. The normality of the distributions can be a concern in this case. 
  
#### Constant variance    
  
The last assumption is that the variance in the groups is about equal from one group to the next. This assumption can be checked by examining a side-by-side box plot of the tipping frequency across the week days.

```{r}
boxplot(uber_shifts$tip_frequency ~ uber_shifts$weekday, col="orange",  main = "Tip frequency distribution by week day", ylab = "Proportion of trips with tips", xlab = "Week day")
```

In this case, the variability is **similar in the 7 groups but not identical**. We saw in a previous section that the standard deviation varies a bit from one group to the next. Whether these differences are from natural variation is unclear.

*We've checked the outlier for Tuesday when proportion of trips with tips was 1. Indeed, on August 20, 2019, driver made 7 trips and received tips for each of them.*

### Compute one-way ANOVA test

Now, we want to calculate the test results. We can use the F statistic to evaluate the hypotheses in what is called an F test. 

Analysis of variance (ANOVA) is used to test whether the mean outcome differs across 2 or more groups. The test statistic F represents a standardized ratio of variability in the sample means relative to the variability within the groups. If H0 is true and the model assumptions are satisfied, the statistic F follows an F distribution with parameters df1 = k - 1 and df2 = n - k. The upper tail of the F distribution is used to represent the p-value.

```{r}
# Compute the analysis of variance
results_aov <- aov(tip_frequency ~ weekday, data = uber_shifts)

# Summary of the analysis
summary(results_aov)
```

The calculated p-value is 0.399, which is, obviously, larger than 0.05, indicating the evidence is not strong enough to reject the null hypothesis at a significance level of 0.05. That is, **the data do not provide strong evidence that the average tip frequency varies by week day.** The variance we saw could be due to chance.
  
As our analysis didn't show the difference between the groups, we would not proceed with the group to group comparison.   

### Check ANOVA assumptions with other methods

*The following tests have been adopted from the article on STHDA (http://www.sthda.com/english/wiki/one-way-anova-test-in-r). 

The ANOVA test assumes that, the data are normally distributed and the variance across groups are homogeneous. We can check that with some diagnostic plots.

#### Homogenity of variance

The residuals versus fits plot can be used to check the **homogeneity of variances**.

```{r}
# Homogeneity of variances
plot(results_aov, 1)
```

*I'm not sure how to interpret this plot and need to get back to it once I learn more* However, it looks pretty similar to one in the article. It could mean that there is no evident relationships between residuals and fitted values (the mean of each groups), which is good. So, we can assume the homogeneity of variances.

It’s also possible to use **Levene’s test** to check the homogeneity of variances.

```{r echo=FALSE, warning=FALSE, results='hide', message=FALSE, error=FALSE}
if (!require(car)) install.packages("car", repos="https://cran.r-project.org/")
library(car)
```

```{r}
leveneTest(tip_frequency ~ weekday, center=median, data = uber_shifts)
```


From the output above we can see that the p-value is not less than the significance level of 0.05. This means that there is no evidence to suggest that the variance across groups is statistically significantly different. Therefore, we can assume the homogeneity of variances in the different treatment groups.

The Levene's test is not significant. The homogeneity is fine.

#### Normality

Normality plot of residuals. In the plot below, the quantiles of the residuals are plotted against the quantiles of the normal distribution. A 45-degree reference line is also plotted.

The normal probability plot of residuals is used to check the assumption that the residuals are normally distributed. It should approximately follow a straight line.

```{r}
# Normality
plot(results_aov, 2)
```

In fact, we see the signs of a right skew here (points bend up and to the left of the line). As the distribution is not perfectly normal, it would be handy to run a **Shapiro-Wilk test** on the ANOVA residuals. 

```{r}
# Extract the residuals
aov_residuals <- residuals(object = results_aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals )
```

The p-value is lower than 0.05 significance level, which means that there's less than 1% chance of seeing this distribution if it was in fact normal. Thus, *the normality assumption is not satisfied.*


### Non-parametric alternative to one-way ANOVA test

There are concerns about normality in our analysis. We can try using a non-parametric alternative to a one-way ANOVA -  **Kruskal-Wallis** rank sum test, which can be used when ANOVA assumptions are not met.

```{r}
kruskal.test(tip_frequency ~ weekday, data = uber_shifts)
```

Employing a Kruskal-Wallis test, we've got a p-value of 0.4073, which is significantly larger than 0.05, meaning that there's not enough evidence to reject the null hypothesis. This result is very close to the p-value from the ANOVA test (0.399).

### Conclusion for the tip frequency analysis

We run ANOVA and Kruskal-Wallis tests to check the hypothesis that UBER riders tip more often on certain days of the week. Neither test could provide significant evidence to reject the null hypothesis at 5% significance level. 

We then conclude that there's in fact no difference between the week days if we talk about how often riders tip this particular driver. Otherwise, we could make a Type II error and failed to reject the null hypothesis when in fact riders tip more often on certain week days.

There are concerns that should be mentioned. First, the normality of the distribution. Second, the sample size for some of the groups (for example, we only have data for 19 Mondays). 

----------------------

# Fare Level problem

In National Tipping Field Experiment, Chandar, et al. (2019)  report that "riders are more likely to tip as the fare of the trip increses, but at a decreasing rate."

**Does our data aligns with this statement? Is there an association between the fare level and tip size?**

```{r}
uber_trips_clean %>%
  filter(is.na(Cancellation)) %>% #filtering for cancelled trips
  mutate(Tip =  ifelse(is.na(Tip), 0, Tip)) %>%
  mutate(Total = round(Total-Tip)) %>% #deducting total by tip amount, rounding total to nearest dollar
  filter(Tip > 0, Total >= 2) %>% #filtering for trips that were not tipped, and for those with a total less than $2 (those are probably just adjastments for previous trips and not the actual rides as the minimal fare for any trip is between $2 and $3)
  group_by(Total) %>% #creating bins for different amounts of total fare
  filter(n() > 9) %>% #filtering for bins with less than 10 trips inside
  summarize(mean_tip = mean(Tip)) %>%
  ggplot(aes(x = Total, y = mean_tip)) +
  geom_point() +
  geom_line(col = "orange") +
  geom_text(aes(label= Total), hjust = -0.2, vjust = 0) +
  labs(title = "Average tip conditional on tipping by trip fare, \nrounded to the nearest dollar", y = "Mean tip in USD (cond. on tipping)", x = "Trip total fare (grouped by dollar)")
```

```{r}
uber_trips_clean %>%
  filter(is.na(Cancellation)) %>% #see previous code chunk for explanations
  mutate(Tip =  ifelse(is.na(Tip), 0, Tip)) %>% #replacing all NAs with 0
  mutate(Total = round(Total-Tip)) %>% #deducting tips from total
  filter(Total >= 2) %>% #filtering for trips with total fare less than $2
  group_by(Total) %>%
  filter(n() > 9) %>%
  summarize(mean_tip = mean(Tip)) %>%
  ggplot(aes(x = Total, y = mean_tip)) +
  geom_point() +
  geom_line(col = "orange") +
  geom_text(aes(label= Total), hjust = -0.2, vjust = 0) +
  labs(title = "Average tip by trip fare, rounded to the nearest dollar", y = "Mean tip in USD", x = "Trip total fare (grouped by dollar)")
```



```{r}
uber_trips_clean %>%
  mutate(Tip =  ifelse(is.na(Tip), 0, Tip), Total = round(Total-Tip)) %>%
  filter(is.na(Cancellation), Total > 2) %>%
  group_by(Total) %>%
  filter(n() > 9) %>%
  summarize(trips_w_tips = sum(Tip>0), total_trips = n(), tips_frequency = trips_w_tips/n()) %>%
  ggplot(aes(x = Total, y = tips_frequency)) +
  geom_point() +
  geom_line(col = "orange") +
  geom_text(aes(label= Total), hjust = -0.2, vjust = 0) +
  labs(title = "Percent of trips tipped by trip fare, \nrounded to the nearest dollar", y = "Percent of trips tipped", x = "Trip total fare (grouped by dollar)")
```

If we had to build the confidence intervals for each of total fare bins, we had to exclude most of the bins that don't meet 10 success/failures condition. On average, we have success rate between 25% and 30%. So, only bins with over 40 and over 30 trips respectively would potentially fit this condition.

## Linear regression model

```{r}
uber_trips_lm <- uber_trips_clean %>%
  mutate(Tip =  ifelse(is.na(Tip), 0, Tip)) %>%
  mutate(Total_minus_tip = (Total-Tip)) %>%
  filter(is.na(Cancellation), Total_minus_tip > 2, Tip > 0)


tip_trip_lm <- lm(Tip ~ Total_minus_tip, data=uber_trips_lm)

summary(tip_trip_lm)  

plot(tip_trip_lm$residuals)
  
```

Looks pretty random to me.



